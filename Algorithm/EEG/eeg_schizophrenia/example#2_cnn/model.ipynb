{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchsummary import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_1='./samples/training.pkl'\n",
    "file_path_2='./samples/testing.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedCustomDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        with open(file_path, 'rb') as f:\n",
    "            self.data = pickle.load(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features, label = self.data[idx]\n",
    "        features = torch.FloatTensor(features)\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CombinedCustomDataset(file_path_1)\n",
    "test_dataset = CombinedCustomDataset(file_path_2)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_Model(nn.Module):\n",
    "    def __init__(self, in_channel=2, input_length=6250):\n",
    "        super(Train_Model,self).__init__()\n",
    "        self.c1=nn.Conv1d(in_channel,out_channels=5,kernel_size=10,stride=1)\n",
    "        self.b1=nn.BatchNorm1d(5)\n",
    "        self.p1=nn.MaxPool1d(kernel_size=2,stride=2)\n",
    "        self.c2=nn.Conv1d(in_channels=5,out_channels=10,kernel_size=10,stride=1)\n",
    "        self.p2=nn.MaxPool1d(kernel_size=2,stride=2)\n",
    "        self.c3 = nn.Conv1d(10, 10, kernel_size=10, stride=1)\n",
    "        self.p3 = nn.AvgPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.c4 = nn.Conv1d(10, 10, kernel_size=10, stride=1)\n",
    "        self.p4 = nn.AvgPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.c5 = nn.Conv1d(10, 15, kernel_size=10, stride=1)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.flatten=nn.Flatten()\n",
    "\n",
    "\n",
    "        self.fc1=nn.Linear(15,10)\n",
    "        self.fc2=nn.Linear(10,5)\n",
    "        self.output=nn.Linear(5,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.c1(x)\n",
    "        x=self.b1(x)\n",
    "        x=nn.LeakyReLU()(x)\n",
    "        x = self.p1(x)\n",
    "        \n",
    "        x = self.c2(x)\n",
    "        x = nn.LeakyReLU()(x)\n",
    "        x = self.p2(x)\n",
    "        \n",
    "        x = self.c3(x)\n",
    "        x = nn.LeakyReLU()(x)\n",
    "        x = self.p3(x)\n",
    "        \n",
    "        x = self.c4(x)\n",
    "        x = nn.LeakyReLU()(x)\n",
    "        x = self.p4(x)\n",
    "        \n",
    "        x = self.c5(x)\n",
    "        x = nn.LeakyReLU()(x)\n",
    "        x = self.global_avg_pool(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = nn.LeakyReLU()(x)\n",
    "        x = self.fc2(x)\n",
    "        x = nn.LeakyReLU()(x)\n",
    "        \n",
    "        x = torch.sigmoid(self.output(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "model=Train_Model(in_channel=2,input_length=7500)\n",
    "criterian=nn.BCELoss()\n",
    "optimizers=torch.optim.Adam(model.parameters(),lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1              [-1, 5, 6241]             105\n",
      "       BatchNorm1d-2              [-1, 5, 6241]              10\n",
      "         MaxPool1d-3              [-1, 5, 3120]               0\n",
      "            Conv1d-4             [-1, 10, 3111]             510\n",
      "         MaxPool1d-5             [-1, 10, 1555]               0\n",
      "            Conv1d-6             [-1, 10, 1546]           1,010\n",
      "         AvgPool1d-7              [-1, 10, 773]               0\n",
      "            Conv1d-8              [-1, 10, 764]           1,010\n",
      "         AvgPool1d-9              [-1, 10, 382]               0\n",
      "           Conv1d-10              [-1, 15, 373]           1,515\n",
      "AdaptiveAvgPool1d-11                [-1, 15, 1]               0\n",
      "          Flatten-12                   [-1, 15]               0\n",
      "          Dropout-13                   [-1, 15]               0\n",
      "           Linear-14                   [-1, 10]             160\n",
      "           Linear-15                    [-1, 5]              55\n",
      "           Linear-16                    [-1, 1]               6\n",
      "================================================================\n",
      "Total params: 4,381\n",
      "Trainable params: 4,381\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 1.26\n",
      "Params size (MB): 0.02\n",
      "Estimated Total Size (MB): 1.32\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model=Train_Model(in_channel=2,input_length=6250)\n",
    "summary(model, (2, 6250)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0/70:  Loss: 0.694423496723175\n",
      "epoch 1/70:  Loss: 0.6884756088256836\n",
      "epoch 2/70:  Loss: 0.6925452947616577\n",
      "epoch 3/70:  Loss: 0.6909406781196594\n",
      "epoch 4/70:  Loss: 0.6948099732398987\n",
      "epoch 5/70:  Loss: 0.6933664083480835\n",
      "epoch 6/70:  Loss: 0.6896024346351624\n",
      "epoch 7/70:  Loss: 0.6932126879692078\n",
      "epoch 8/70:  Loss: 0.6932287812232971\n",
      "epoch 9/70:  Loss: 0.6938400268554688\n",
      "epoch 10/70:  Loss: 0.6870933771133423\n",
      "epoch 11/70:  Loss: 0.6926371455192566\n",
      "epoch 12/70:  Loss: 0.6949718594551086\n",
      "epoch 13/70:  Loss: 0.6915622353553772\n",
      "epoch 14/70:  Loss: 0.6952152252197266\n",
      "epoch 15/70:  Loss: 0.6901819109916687\n",
      "epoch 16/70:  Loss: 0.6952898502349854\n",
      "epoch 17/70:  Loss: 0.6975070238113403\n",
      "epoch 18/70:  Loss: 0.7014557123184204\n",
      "epoch 19/70:  Loss: 0.6951996684074402\n",
      "epoch 20/70:  Loss: 0.6935819387435913\n",
      "epoch 21/70:  Loss: 0.694301426410675\n",
      "epoch 22/70:  Loss: 0.6894053816795349\n",
      "epoch 23/70:  Loss: 0.6975651383399963\n",
      "epoch 24/70:  Loss: 0.6952767968177795\n",
      "epoch 25/70:  Loss: 0.7009759545326233\n",
      "epoch 26/70:  Loss: 0.697881281375885\n",
      "epoch 27/70:  Loss: 0.6895784735679626\n",
      "epoch 28/70:  Loss: 0.6899527907371521\n",
      "epoch 29/70:  Loss: 0.688530445098877\n",
      "epoch 30/70:  Loss: 0.698354184627533\n",
      "epoch 31/70:  Loss: 0.6943188309669495\n",
      "epoch 32/70:  Loss: 0.6933948397636414\n",
      "epoch 33/70:  Loss: 0.692827582359314\n",
      "epoch 34/70:  Loss: 0.6858072876930237\n",
      "epoch 35/70:  Loss: 0.6894422173500061\n",
      "epoch 36/70:  Loss: 0.6928335428237915\n",
      "epoch 37/70:  Loss: 0.698986828327179\n",
      "epoch 38/70:  Loss: 0.7000569701194763\n",
      "epoch 39/70:  Loss: 0.6899338364601135\n",
      "epoch 40/70:  Loss: 0.6982150077819824\n",
      "epoch 41/70:  Loss: 0.6962727904319763\n",
      "epoch 42/70:  Loss: 0.6944549679756165\n",
      "epoch 43/70:  Loss: 0.694360077381134\n",
      "epoch 44/70:  Loss: 0.694199800491333\n",
      "epoch 45/70:  Loss: 0.6890119314193726\n",
      "epoch 46/70:  Loss: 0.6935935616493225\n",
      "epoch 47/70:  Loss: 0.6901746392250061\n",
      "epoch 48/70:  Loss: 0.6925599575042725\n",
      "epoch 49/70:  Loss: 0.6966047883033752\n",
      "epoch 50/70:  Loss: 0.6946367621421814\n",
      "epoch 51/70:  Loss: 0.6894500851631165\n",
      "epoch 52/70:  Loss: 0.698307454586029\n",
      "epoch 53/70:  Loss: 0.6940549612045288\n",
      "epoch 54/70:  Loss: 0.6923274397850037\n",
      "epoch 55/70:  Loss: 0.695138156414032\n",
      "epoch 56/70:  Loss: 0.6957492232322693\n",
      "epoch 57/70:  Loss: 0.6977866888046265\n",
      "epoch 58/70:  Loss: 0.6942150592803955\n",
      "epoch 59/70:  Loss: 0.6949233412742615\n",
      "epoch 60/70:  Loss: 0.6955681443214417\n",
      "epoch 61/70:  Loss: 0.6944049596786499\n",
      "epoch 62/70:  Loss: 0.6946402192115784\n",
      "epoch 63/70:  Loss: 0.6898887753486633\n",
      "epoch 64/70:  Loss: 0.6964673399925232\n",
      "epoch 65/70:  Loss: 0.6929194927215576\n",
      "epoch 66/70:  Loss: 0.6896259784698486\n",
      "epoch 67/70:  Loss: 0.6970604658126831\n",
      "epoch 68/70:  Loss: 0.6955702304840088\n",
      "epoch 69/70:  Loss: 0.6932567954063416\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "epochs=30\n",
    "losses=[]\n",
    "for epoch in range(epochs):\n",
    "    for inputs,outputs in train_loader:\n",
    "        y_pred=model.forward(inputs).squeeze()\n",
    "        loss=criterian(y_pred,outputs)\n",
    "        losses.append(loss)\n",
    "        optimizers.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizers.step()\n",
    "\n",
    "    print('epoch {}/{}:  Loss: {}'.format(epoch,epochs,loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.7244420051574707, Average Accuracy: 0.49857954545454547\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "total_accuracy=0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        test_preds = model(inputs).squeeze()  # Squeeze to ensure dimension match\n",
    "        test_loss = criterian(test_preds, targets)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        predicted = test_preds.round()  # Assuming threshold of 0.5\n",
    "        accuracy = (predicted == targets).float().mean().item()\n",
    "        total_accuracy += accuracy\n",
    "\n",
    "    avg_accuracy = total_accuracy / len(test_loader)\n",
    "    print(f'Test Loss: {test_loss.item()}, Average Accuracy: {avg_accuracy}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'Best_Model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
