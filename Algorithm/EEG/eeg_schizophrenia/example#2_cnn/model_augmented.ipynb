{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader,Dataset,SequentialSampler,BatchSampler\n",
    "from torchsummary import summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path1 = './samples/pos_class_train.pkl'\n",
    "file_path2 = './samples/neg_class_train.pkl'\n",
    "file_path3='./samples/testing_augmented.pkl'\n",
    "\n",
    "with open(file_path1, 'rb') as f:\n",
    "    # Load the contents of the pickle file\n",
    "    pos_class_train = pickle.load(f)\n",
    "\n",
    "with open(file_path2, 'rb') as f:\n",
    "    # Load the contents of the pickle file\n",
    "    neg_class_train = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOversampledDataset(Dataset):\n",
    "    def __init__(self, class_a, class_b):\n",
    "        # Assume class_a and class_b are lists of tuples (features, label)\n",
    "        if len(class_a) > len(class_b):\n",
    "            self.majority_class = class_a\n",
    "            self.minority_class = class_b\n",
    "        else:\n",
    "            self.majority_class = class_b\n",
    "            self.minority_class = class_a\n",
    "\n",
    "        self.oversample_minority()\n",
    "\n",
    "    def oversample_minority(self):\n",
    "        # Calculate how many times to repeat the minority class\n",
    "        repeat_times = len(self.majority_class) // len(self.minority_class)\n",
    "        self.data = self.majority_class + self.minority_class * repeat_times\n",
    "        # Add remainder if necessary\n",
    "        remainder = len(self.majority_class) % len(self.minority_class)\n",
    "        if remainder:\n",
    "            self.data += self.minority_class[:remainder]\n",
    "        \n",
    "        # Shuffle to mix the samples using torch.randperm\n",
    "        perm = torch.randperm(len(self.data))\n",
    "        self.data = [self.data[i] for i in perm]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features, label = self.data[idx]\n",
    "        return torch.tensor(features, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonRepeatingBatchSampler(BatchSampler):\n",
    "    def __init__(self, data_source, batch_size, drop_last=True):\n",
    "        super().__init__(SequentialSampler(data_source), batch_size, drop_last)\n",
    "        self.data_source = data_source\n",
    "\n",
    "    def __iter__(self):\n",
    "        batch = []\n",
    "        seen_ids = set()\n",
    "        for idx in torch.randperm(len(self.data_source)):\n",
    "            sample_id = id(self.data_source.data[idx])\n",
    "            if sample_id not in seen_ids or len(seen_ids) == len(self.data_source): # Check if all samples have been seen\n",
    "                batch.append(idx)\n",
    "                seen_ids.add(sample_id)\n",
    "                if len(batch) == self.batch_size:\n",
    "                    yield batch\n",
    "                    batch = []\n",
    "                    seen_ids = set()\n",
    "        if batch and not self.drop_last:\n",
    "            yield batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomOversampledDataset(pos_class_train, neg_class_train)\n",
    "sampler = NonRepeatingBatchSampler(dataset, batch_size=32)\n",
    "train_loader = DataLoader(dataset, batch_sampler=sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedCustomDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        with open(file_path, 'rb') as f:\n",
    "            self.data = pickle.load(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features, label = self.data[idx]\n",
    "        features = torch.FloatTensor(features)\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        return features, label\n",
    "test_dataset = CombinedCustomDataset(file_path3)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_Model(nn.Module):\n",
    "    def __init__(self, in_channel=2, input_length=6250):\n",
    "        super(Train_Model,self).__init__()\n",
    "        self.c1=nn.Conv1d(in_channel,out_channels=5,kernel_size=10,stride=1)\n",
    "        self.p1=nn.MaxPool1d(kernel_size=2,stride=2)\n",
    "        self.c2=nn.Conv1d(in_channels=5,out_channels=10,kernel_size=10,stride=1)\n",
    "        self.p2=nn.MaxPool1d(kernel_size=2,stride=2)\n",
    "        self.c3 = nn.Conv1d(10, 10, kernel_size=10, stride=1)\n",
    "        self.p3 =nn.MaxPool1d(kernel_size=2,stride=2)\n",
    "        self.c4 = nn.Conv1d(10, 15, kernel_size=5, stride=1)\n",
    "        self.p4 =nn.MaxPool1d(kernel_size=2,stride=2)\n",
    "        self.flatten=nn.Flatten()\n",
    "\n",
    "\n",
    "        self.fc1=nn.Linear(5760,64)\n",
    "        self.fc2=nn.Linear(64,20)\n",
    "        self.output=nn.Linear(20,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.c1(x)\n",
    "        x = self.p1(x)\n",
    "        x=nn.LeakyReLU()(x)\n",
    "        x = self.c2(x)\n",
    "        x = nn.LeakyReLU()(x)\n",
    "        x = self.p2(x)\n",
    "        x = self.c3(x)\n",
    "        x = nn.LeakyReLU()(x)\n",
    "        x = self.p3(x)\n",
    "        x = self.c4(x)\n",
    "        x = nn.LeakyReLU()(x)\n",
    "        x = self.p4(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.LeakyReLU()(x)\n",
    "        x = self.fc2(x)\n",
    "        x = nn.LeakyReLU()(x)\n",
    "        x = torch.sigmoid(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1              [-1, 5, 6241]             105\n",
      "         MaxPool1d-2              [-1, 5, 3120]               0\n",
      "            Conv1d-3             [-1, 10, 3111]             510\n",
      "         MaxPool1d-4             [-1, 10, 1555]               0\n",
      "            Conv1d-5             [-1, 10, 1546]           1,010\n",
      "         MaxPool1d-6              [-1, 10, 773]               0\n",
      "            Conv1d-7              [-1, 15, 769]             765\n",
      "         MaxPool1d-8              [-1, 15, 384]               0\n",
      "           Flatten-9                 [-1, 5760]               0\n",
      "           Linear-10                   [-1, 64]         368,704\n",
      "           Linear-11                   [-1, 20]           1,300\n",
      "           Linear-12                    [-1, 1]              21\n",
      "================================================================\n",
      "Total params: 372,415\n",
      "Trainable params: 372,415\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 1.07\n",
      "Params size (MB): 1.42\n",
      "Estimated Total Size (MB): 2.53\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model=Train_Model(in_channel=2,input_length=6250)\n",
    "criterian=nn.BCELoss()\n",
    "optimizers=torch.optim.Adam(model.parameters(),lr=0.0001)\n",
    "summary(model, (2, 6250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0/70:  Loss: 0.6958727836608887\n",
      "epoch 1/70:  Loss: 0.6359607577323914\n",
      "epoch 2/70:  Loss: 0.5279501676559448\n",
      "epoch 3/70:  Loss: 0.5587435960769653\n",
      "epoch 4/70:  Loss: 0.5982241630554199\n",
      "epoch 5/70:  Loss: 0.5619750618934631\n",
      "epoch 6/70:  Loss: 0.5821767449378967\n",
      "epoch 7/70:  Loss: 0.49157285690307617\n",
      "epoch 8/70:  Loss: 0.44785910844802856\n",
      "epoch 9/70:  Loss: 0.43653279542922974\n",
      "epoch 10/70:  Loss: 0.3752642273902893\n",
      "epoch 11/70:  Loss: 0.4658612608909607\n",
      "epoch 12/70:  Loss: 0.42323076725006104\n",
      "epoch 13/70:  Loss: 0.5785524845123291\n",
      "epoch 14/70:  Loss: 0.41239941120147705\n",
      "epoch 15/70:  Loss: 0.2316867560148239\n",
      "epoch 16/70:  Loss: 0.39910703897476196\n",
      "epoch 17/70:  Loss: 0.39036881923675537\n",
      "epoch 18/70:  Loss: 0.49795085191726685\n",
      "epoch 19/70:  Loss: 0.4272502064704895\n",
      "epoch 20/70:  Loss: 0.5759829878807068\n",
      "epoch 21/70:  Loss: 0.3793007433414459\n",
      "epoch 22/70:  Loss: 0.41361236572265625\n",
      "epoch 23/70:  Loss: 0.4744502604007721\n",
      "epoch 24/70:  Loss: 0.3662857413291931\n",
      "epoch 25/70:  Loss: 0.4179553985595703\n",
      "epoch 26/70:  Loss: 0.38129040598869324\n",
      "epoch 27/70:  Loss: 0.3655710518360138\n",
      "epoch 28/70:  Loss: 0.331868052482605\n",
      "epoch 29/70:  Loss: 0.39295223355293274\n",
      "epoch 30/70:  Loss: 0.41095247864723206\n",
      "epoch 31/70:  Loss: 0.30457401275634766\n",
      "epoch 32/70:  Loss: 0.4916057884693146\n",
      "epoch 33/70:  Loss: 0.35130178928375244\n",
      "epoch 34/70:  Loss: 0.4677158296108246\n",
      "epoch 35/70:  Loss: 0.46897760033607483\n",
      "epoch 36/70:  Loss: 0.357997328042984\n",
      "epoch 37/70:  Loss: 0.4613170623779297\n",
      "epoch 38/70:  Loss: 0.3427172005176544\n",
      "epoch 39/70:  Loss: 0.4847415089607239\n",
      "epoch 40/70:  Loss: 0.34511828422546387\n",
      "epoch 41/70:  Loss: 0.3412652611732483\n",
      "epoch 42/70:  Loss: 0.47291117906570435\n",
      "epoch 43/70:  Loss: 0.40686744451522827\n",
      "epoch 44/70:  Loss: 0.33672016859054565\n",
      "epoch 45/70:  Loss: 0.3732284903526306\n",
      "epoch 46/70:  Loss: 0.33207011222839355\n",
      "epoch 47/70:  Loss: 0.540505051612854\n",
      "epoch 48/70:  Loss: 0.4047073721885681\n",
      "epoch 49/70:  Loss: 0.35317352414131165\n",
      "epoch 50/70:  Loss: 0.314098984003067\n",
      "epoch 51/70:  Loss: 0.5155067443847656\n",
      "epoch 52/70:  Loss: 0.35658472776412964\n",
      "epoch 53/70:  Loss: 0.4543752074241638\n",
      "epoch 54/70:  Loss: 0.39264488220214844\n",
      "epoch 55/70:  Loss: 0.4094282388687134\n",
      "epoch 56/70:  Loss: 0.36512690782546997\n",
      "epoch 57/70:  Loss: 0.4463695287704468\n",
      "epoch 58/70:  Loss: 0.28648489713668823\n",
      "epoch 59/70:  Loss: 0.3957884609699249\n",
      "epoch 60/70:  Loss: 0.4847467541694641\n",
      "epoch 61/70:  Loss: 0.2986011505126953\n",
      "epoch 62/70:  Loss: 0.5605506896972656\n",
      "epoch 63/70:  Loss: 0.3561372756958008\n",
      "epoch 64/70:  Loss: 0.285808801651001\n",
      "epoch 65/70:  Loss: 0.39433756470680237\n",
      "epoch 66/70:  Loss: 0.32039013504981995\n",
      "epoch 67/70:  Loss: 0.44108301401138306\n",
      "epoch 68/70:  Loss: 0.32631915807724\n",
      "epoch 69/70:  Loss: 0.3300652503967285\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "epochs=70\n",
    "losses=[]\n",
    "for epoch in range(epochs):\n",
    "    for inputs,outputs in train_loader:\n",
    "        outputs=outputs.float()\n",
    "        y_pred=model.forward(inputs).squeeze()\n",
    "        loss=criterian(y_pred,outputs)\n",
    "        losses.append(loss)\n",
    "        optimizers.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizers.step()\n",
    "\n",
    "    print('epoch {}/{}:  Loss: {}'.format(epoch,epochs,loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.29196977615356445, Average Accuracy: 0.7043269230769231\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "total_accuracy=0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        test_preds = model(inputs).squeeze()  # Squeeze to ensure dimension match\n",
    "        test_loss = criterian(test_preds, targets)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        predicted = test_preds.round()  # Assuming threshold of 0.5\n",
    "        accuracy = (predicted == targets).float().mean().item()\n",
    "        total_accuracy += accuracy\n",
    "\n",
    "    avg_accuracy = total_accuracy / len(test_loader)\n",
    "    print(f'Test Loss: {test_loss.item()}, Average Accuracy: {avg_accuracy}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
