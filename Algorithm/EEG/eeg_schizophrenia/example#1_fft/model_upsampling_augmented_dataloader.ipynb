{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader,Dataset,SequentialSampler,BatchSampler\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path1 = './samples/pos_class_train.pkl'\n",
    "file_path2 = './samples/neg_class_train.pkl'\n",
    "file_path3='./samples/testing_data.pkl'\n",
    "\n",
    "with open(file_path1, 'rb') as f:\n",
    "    # Load the contents of the pickle file\n",
    "    pos_class_train = pickle.load(f)\n",
    "\n",
    "with open(file_path2, 'rb') as f:\n",
    "    # Load the contents of the pickle file\n",
    "    neg_class_train = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOversampledDataset(Dataset):\n",
    "    def __init__(self, class_a, class_b):\n",
    "        # Assume class_a and class_b are lists of tuples (features, label)\n",
    "        if len(class_a) > len(class_b):\n",
    "            self.majority_class = class_a\n",
    "            self.minority_class = class_b\n",
    "        else:\n",
    "            self.majority_class = class_b\n",
    "            self.minority_class = class_a\n",
    "\n",
    "        self.oversample_minority()\n",
    "\n",
    "    def oversample_minority(self):\n",
    "        # Calculate how many times to repeat the minority class\n",
    "        repeat_times = len(self.majority_class) // len(self.minority_class)\n",
    "        self.data = self.majority_class + self.minority_class * repeat_times\n",
    "        # Add remainder if necessary\n",
    "        remainder = len(self.majority_class) % len(self.minority_class)\n",
    "        if remainder:\n",
    "            self.data += self.minority_class[:remainder]\n",
    "        \n",
    "        # Shuffle to mix the samples using torch.randperm\n",
    "        perm = torch.randperm(len(self.data))\n",
    "        self.data = [self.data[i] for i in perm]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features, label = self.data[idx]\n",
    "        return torch.tensor(features, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonRepeatingBatchSampler(BatchSampler):\n",
    "    def __init__(self, data_source, batch_size, drop_last=True):\n",
    "        super().__init__(SequentialSampler(data_source), batch_size, drop_last)\n",
    "        self.data_source = data_source\n",
    "\n",
    "    def __iter__(self):\n",
    "        batch = []\n",
    "        seen_ids = set()\n",
    "        for idx in torch.randperm(len(self.data_source)):\n",
    "            sample_id = id(self.data_source.data[idx])\n",
    "            if sample_id not in seen_ids or len(seen_ids) == len(self.data_source): # Check if all samples have been seen\n",
    "                batch.append(idx)\n",
    "                seen_ids.add(sample_id)\n",
    "                if len(batch) == self.batch_size:\n",
    "                    yield batch\n",
    "                    batch = []\n",
    "                    seen_ids = set()\n",
    "        if batch and not self.drop_last:\n",
    "            yield batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomOversampledDataset(pos_class_train, neg_class_train)\n",
    "sampler = NonRepeatingBatchSampler(dataset, batch_size=32)\n",
    "train_loader = DataLoader(dataset, batch_sampler=sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedCustomDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        with open(file_path, 'rb') as f:\n",
    "            self.data = pickle.load(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features, label = self.data[idx]\n",
    "        features = torch.FloatTensor(features)\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        return features, label\n",
    "test_dataset = CombinedCustomDataset(file_path3)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "model = NeuralNetwork(800)\n",
    "criterian = nn.BCELoss()\n",
    "optimizers = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0/70:  Loss: 0.5962590575218201\n",
      "epoch 1/70:  Loss: 0.5711008906364441\n",
      "epoch 2/70:  Loss: 0.7440171837806702\n",
      "epoch 3/70:  Loss: 0.5756639242172241\n",
      "epoch 4/70:  Loss: 0.1681673228740692\n",
      "epoch 5/70:  Loss: 0.4336845278739929\n",
      "epoch 6/70:  Loss: 0.23520123958587646\n",
      "epoch 7/70:  Loss: 0.36252841353416443\n",
      "epoch 8/70:  Loss: 0.18301606178283691\n",
      "epoch 9/70:  Loss: 0.16134323179721832\n",
      "epoch 10/70:  Loss: 0.14620748162269592\n",
      "epoch 11/70:  Loss: 0.261696994304657\n",
      "epoch 12/70:  Loss: 0.33758658170700073\n",
      "epoch 13/70:  Loss: 0.41977033019065857\n",
      "epoch 14/70:  Loss: 0.07452325522899628\n",
      "epoch 15/70:  Loss: 0.22560684382915497\n",
      "epoch 16/70:  Loss: 0.13375405967235565\n",
      "epoch 17/70:  Loss: 0.09791721403598785\n",
      "epoch 18/70:  Loss: 0.24616611003875732\n",
      "epoch 19/70:  Loss: 0.02682715654373169\n",
      "epoch 20/70:  Loss: 0.06648076325654984\n",
      "epoch 21/70:  Loss: 0.04572266340255737\n",
      "epoch 22/70:  Loss: 0.06912669539451599\n",
      "epoch 23/70:  Loss: 0.02961690165102482\n",
      "epoch 24/70:  Loss: 0.01137927733361721\n",
      "epoch 25/70:  Loss: 0.0004654137883335352\n",
      "epoch 26/70:  Loss: 0.02014515921473503\n",
      "epoch 27/70:  Loss: 0.07292219996452332\n",
      "epoch 28/70:  Loss: 0.021801559254527092\n",
      "epoch 29/70:  Loss: 0.0038546030409634113\n",
      "epoch 30/70:  Loss: 0.006647104863077402\n",
      "epoch 31/70:  Loss: 0.023823749274015427\n",
      "epoch 32/70:  Loss: 0.007188229355961084\n",
      "epoch 33/70:  Loss: 0.006532450206577778\n",
      "epoch 34/70:  Loss: 0.0008795636822469532\n",
      "epoch 35/70:  Loss: 0.004680278245359659\n",
      "epoch 36/70:  Loss: 0.0026347015518695116\n",
      "epoch 37/70:  Loss: 0.001948071992956102\n",
      "epoch 38/70:  Loss: 0.0038393917493522167\n",
      "epoch 39/70:  Loss: 0.0008740561897866428\n",
      "epoch 40/70:  Loss: 0.0016541488002985716\n",
      "epoch 41/70:  Loss: 0.0011604407336562872\n",
      "epoch 42/70:  Loss: 0.0039194501005113125\n",
      "epoch 43/70:  Loss: 0.0033269014675170183\n",
      "epoch 44/70:  Loss: 0.0005395672051236033\n",
      "epoch 45/70:  Loss: 0.0013923047808930278\n",
      "epoch 46/70:  Loss: 0.00028663183911703527\n",
      "epoch 47/70:  Loss: 0.0009314814815297723\n",
      "epoch 48/70:  Loss: 0.0008907847804948688\n",
      "epoch 49/70:  Loss: 0.00032969581661745906\n",
      "epoch 50/70:  Loss: 0.0012848752085119486\n",
      "epoch 51/70:  Loss: 5.587920168181881e-05\n",
      "epoch 52/70:  Loss: 0.0014706712681800127\n",
      "epoch 53/70:  Loss: 0.001213995274156332\n",
      "epoch 54/70:  Loss: 5.3287919854483334e-08\n",
      "epoch 55/70:  Loss: 0.0012096419231966138\n",
      "epoch 56/70:  Loss: 8.569721831008792e-05\n",
      "epoch 57/70:  Loss: 0.00016306813631672412\n",
      "epoch 58/70:  Loss: 0.00023725061328150332\n",
      "epoch 59/70:  Loss: 0.0003755927609745413\n",
      "epoch 60/70:  Loss: 0.0008039803942665458\n",
      "epoch 61/70:  Loss: 9.594175935490057e-05\n",
      "epoch 62/70:  Loss: 0.0006871685618534684\n",
      "epoch 63/70:  Loss: 0.0009042203309945762\n",
      "epoch 64/70:  Loss: 0.0004400973557494581\n",
      "epoch 65/70:  Loss: 6.08311383984983e-05\n",
      "epoch 66/70:  Loss: 0.0005273475544527173\n",
      "epoch 67/70:  Loss: 0.00015457047265954316\n",
      "epoch 68/70:  Loss: 0.00021547582582570612\n",
      "epoch 69/70:  Loss: 0.00018331236788071692\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "epochs=70\n",
    "losses=[]\n",
    "for epoch in range(epochs):\n",
    "    for inputs,outputs in train_loader:\n",
    "        y_pred=model.forward(inputs).squeeze()\n",
    "        outputs=outputs.float()\n",
    "        loss=criterian(y_pred,outputs)\n",
    "        losses.append(loss)\n",
    "        optimizers.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizers.step()\n",
    "\n",
    "    print('epoch {}/{}:  Loss: {}'.format(epoch,epochs,loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.08726920932531357, Average Accuracy: 0.65625\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "total_accuracy=0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        test_preds = model(inputs).squeeze() \n",
    "        test_loss = criterian(test_preds, targets)\n",
    "        predicted = test_preds.round() \n",
    "        accuracy = (predicted == targets).float().mean().item()\n",
    "        total_accuracy += accuracy\n",
    "\n",
    "    avg_accuracy = total_accuracy / len(test_loader)\n",
    "    print(f'Test Loss: {test_loss.item()}, Average Accuracy: {avg_accuracy}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
