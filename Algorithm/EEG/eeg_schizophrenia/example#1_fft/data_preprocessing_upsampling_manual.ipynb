{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import mne\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from os.path import exists\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sampling_Frequency=250\n",
    "#Length_of_trail=25\n",
    "k=400\n",
    "healthy=14\n",
    "schizo=14\n",
    "file_name=['h','s']\n",
    "Band_Pass_Low_Range=0\n",
    "Band_Pass_High_Range=50\n",
    "iir_params = {'order': 2, 'ftype': 'butter'}\n",
    "recording_time=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Subjects shuffling\n",
    "indice=list(range(1,healthy+1))\n",
    "random.shuffle(indice)\n",
    "indice=[1, 12, 5, 11, 13, 10, 2, 9, 6, 8, 4, 3, 7, 14]\n",
    "print(indice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_channel(raw,selected_channel=['Fp1','Fp2']):\n",
    "    channel_indices=[]\n",
    "    for channel in selected_channel:\n",
    "        try:\n",
    "            index=raw.ch_names.index(channel)\n",
    "            channel_indices.append(index)\n",
    "        except ValueError:\n",
    "            print('Channel{} is not present or again check the names of channel'.format())\n",
    "    new_raw=raw.get_data(picks=channel_indices)\n",
    "    return new_raw\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_augmentation(data, window_size, stride):\n",
    "    data=np.array(data)\n",
    "    n_channels,n_timepoints,  = data.shape\n",
    "    augmented_data = []\n",
    "    for start in range(0, n_timepoints - window_size + 1, stride):\n",
    "        end = start + window_size\n",
    "        segment = data[:,start:end]\n",
    "        augmented_data.append(segment)\n",
    "    return np.array(augmented_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def select_features(new_raw, label, samp_freq=Sampling_Frequency, recording_time=25, augment_ratio=2, k=k):\n",
    "    scaler = MinMaxScaler()\n",
    "    trail_length = samp_freq * recording_time   # Calculating Length of each Trail\n",
    "    stride = trail_length // augment_ratio\n",
    "    trail_length_shape = np.shape(new_raw)[1]   # Determining the time of each channel\n",
    "    total_chunks = trail_length_shape // trail_length    # Final How many chunks can be created \n",
    "    new_raw = new_raw[:, 0:total_chunks * trail_length]     # Changing Shape of Data\n",
    "    augmented_eeg_data = sliding_window_augmentation(new_raw, trail_length, stride)\n",
    "    \n",
    "    feature = []\n",
    "    for chunk in augmented_eeg_data:\n",
    "        segg = []\n",
    "        for i in range(chunk.shape[0]):\n",
    "            fourier = np.abs(np.fft.fft(chunk[i, :]))\n",
    "            # Select the top k features based on their scores\n",
    "            selected_features = SelectKBest(score_func=f_classif, k=k)\n",
    "            selected_features.fit_transform(fourier.reshape(1, -1), np.array([label]))  # Assuming label is binary\n",
    "            selected_indices = selected_features.get_support(indices=True)\n",
    "            selected_fourier = fourier[selected_indices]\n",
    "            segg.append(selected_fourier)\n",
    "        final_concate = np.concatenate(segg, axis=0)\n",
    "        final_concate = final_concate.reshape(-1, 1)\n",
    "        final_concate = scaler.fit_transform(final_concate)\n",
    "        final_concate = final_concate.ravel()\n",
    "        feature.append((final_concate, label))\n",
    "    return feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining all and preprocessing the whole data\n",
    "def preprocess(subject_name,indices,labels,number_of_subjects=14,train_percent=0.8,Band_Pass_Low_Range=0,Band_Pass_High_Range=50,trail_times=25,augmented_ratio=2):\n",
    "    limit=math.ceil(train_percent*number_of_subjects)\n",
    "    k=1\n",
    "    train_data=[]\n",
    "    test_data=[]\n",
    "    for subject_number in indices:\n",
    "        if exists('./Dataset/{}{}.edf'.format(subject_name,subject_number)):\n",
    "            file='./Dataset/{}{}.edf'.format(subject_name,subject_number)\n",
    "            raw=mne.io.read_raw_edf(file,verbose=0,preload=True,eog=None,exclude=(),stim_channel='auto')\n",
    "            raw=raw.pick_types(eeg=True)\n",
    "            raw=raw.filter(Band_Pass_Low_Range,Band_Pass_High_Range,iir_params=iir_params,method='iir')\n",
    "            new_raw=select_channel(raw)\n",
    "            #new_raw=normalize_eeg_data(new_raw)\n",
    "            chunks=select_features(new_raw,label=labels,recording_time=trail_times,augment_ratio=augmented_ratio)\n",
    "            if k<=limit:\n",
    "                train_data+=chunks\n",
    "            else:\n",
    "                test_data+=chunks\n",
    "            k=k+1\n",
    "    return train_data,test_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches_division(pos_class_train,neg_class_train,batch_size=32,train_set=True):\n",
    "    if train_set==True:\n",
    "        num_batches=max(len(pos_class_train),len(neg_class_train))//batch_size\n",
    "    else:\n",
    "        num_batches=min(len(pos_class_train),len(neg_class_train))//batch_size\n",
    "\n",
    "    batch_size=batch_size//2\n",
    "    samples_select=batch_size//2\n",
    "    selected_batch = set()\n",
    "    combined_batches = []\n",
    "    pos_batches = [pos_class_train[i * batch_size: (i + 1) * batch_size] for i in range(num_batches)]\n",
    "    neg_batches = [neg_class_train[i * batch_size: (i + 1) * batch_size] for i in range(num_batches)]\n",
    "    if len(pos_class_train)>len(neg_class_train):\n",
    "        pos_batches_filled = copy.deepcopy(neg_batches)\n",
    "    else:\n",
    "        pos_batches_filled = copy.deepcopy(pos_batches) \n",
    "\n",
    "    while any(len(batch) < batch_size for batch in pos_batches_filled):\n",
    "        for i in range(len(pos_batches_filled)):\n",
    "            if len(pos_batches_filled[i]) == 0 or len(pos_batches_filled[i]) < batch_size:\n",
    "                num_of_samples_to_find = batch_size - len(pos_batches_filled[i])\n",
    "                non_empty_batches_indices = [j for j in range(len(pos_batches_filled)) if\n",
    "                                            len(pos_batches_filled[j]) > 0 and len(pos_batches_filled[j]) == batch_size]\n",
    "                random_non_empty_batch_index = np.random.choice(non_empty_batches_indices)\n",
    "                while random_non_empty_batch_index in selected_batch:\n",
    "                    random_non_empty_batch_index = np.random.choice(non_empty_batches_indices)\n",
    "                selected_batch.add(random_non_empty_batch_index)\n",
    "                random_non_empty_batch = pos_batches_filled[random_non_empty_batch_index]\n",
    "                num_samples_to_pick = min(samples_select, num_of_samples_to_find)\n",
    "                random_samples_indices = np.random.choice(range(len(random_non_empty_batch)), num_samples_to_pick,\n",
    "                                                        replace=False)\n",
    "                for index in random_samples_indices:\n",
    "                    pos_batches_filled[i].append(random_non_empty_batch[index])\n",
    "        pos_batches=pos_batches_filled\n",
    "    for pos_batch, neg_batch in zip(pos_batches, neg_batches):\n",
    "        combined_batch = []\n",
    "        for pos_chunk, neg_chunk in zip(pos_batch, neg_batch):\n",
    "            combined_batch.append(pos_chunk)\n",
    "            combined_batch.append(neg_chunk)\n",
    "        random.shuffle(combined_batch)\n",
    "        combined_batches.append(combined_batch)\n",
    "    random.shuffle(combined_batches)\n",
    "\n",
    "\n",
    "    return combined_batches,pos_batches,neg_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_class_train,pos_class_test=preprocess('h',indices=indice,labels=0,trail_times=recording_time,augmented_ratio=3,train_percent=0.8)\n",
    "neg_class_train,neg_class_test=preprocess('s',indices=indice,labels=1,trail_times=recording_time,augmented_ratio=3,train_percent=0.8)\n",
    "training_data,_,_=batches_division(pos_class_train,neg_class_train,batch_size=32)\n",
    "testing_data=random.shuffle(pos_class_test+neg_class_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using double backslashes\n",
    "with open('./samples/training_augmented.pkl', 'wb') as f:\n",
    "    pickle.dump(training_data, f)\n",
    "\n",
    "with open('./samples/testing_augmented.pkl', 'wb') as f:\n",
    "    pickle.dump(testing_data, f)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
