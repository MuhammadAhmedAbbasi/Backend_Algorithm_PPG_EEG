{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import mne\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from os.path import exists\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sampling_Frequency=250\n",
    "#Length_of_trail=25\n",
    "k=400\n",
    "healthy=14\n",
    "schizo=14\n",
    "file_name=['h','s']\n",
    "Band_Pass_Low_Range=0\n",
    "Band_Pass_High_Range=50\n",
    "iir_params = {'order': 2, 'ftype': 'butter'}\n",
    "recording_time=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Subjects shuffling\n",
    "indice=list(range(1,healthy+1))\n",
    "random.shuffle(indice)\n",
    "#indice=[6, 11, 10, 1, 8, 3, 13, 5, 7, 12, 9, 14, 4, 2]\n",
    "indice=[2, 3, 6, 10, 9, 12, 14, 7, 1, 4, 8, 13, 5, 11]\n",
    "print(indice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_channel(raw,selected_channel=['Fp1','Fp2']):\n",
    "    channel_indices=[]\n",
    "    for channel in selected_channel:\n",
    "        try:\n",
    "            index=raw.ch_names.index(channel)\n",
    "            channel_indices.append(index)\n",
    "        except ValueError:\n",
    "            print('Channel{} is not present or again check the names of channel'.format())\n",
    "    new_raw=raw.get_data(picks=channel_indices)\n",
    "    return new_raw\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_augmentation(data, window_size, stride):\n",
    "    data=np.array(data)\n",
    "    n_channels,n_timepoints,  = data.shape\n",
    "    augmented_data = []\n",
    "    for start in range(0, n_timepoints - window_size + 1, stride):\n",
    "        end = start + window_size\n",
    "        segment = data[:,start:end]\n",
    "        augmented_data.append(segment)\n",
    "    return np.array(augmented_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def select_features(new_raw, label, samp_freq=Sampling_Frequency, recording_time=25, augment_ratio=2, k=k):\n",
    "    scaler = MinMaxScaler()\n",
    "    trail_length = samp_freq * recording_time   # Calculating Length of each Trail\n",
    "    stride = trail_length // augment_ratio\n",
    "    trail_length_shape = np.shape(new_raw)[1]   # Determining the time of each channel\n",
    "    total_chunks = trail_length_shape // trail_length    # Final How many chunks can be created \n",
    "    new_raw = new_raw[:, 0:total_chunks * trail_length]     # Changing Shape of Data\n",
    "    augmented_eeg_data = sliding_window_augmentation(new_raw, trail_length, stride)\n",
    "    \n",
    "    feature = []\n",
    "    for chunk in augmented_eeg_data:\n",
    "        segg = []\n",
    "        for i in range(chunk.shape[0]):\n",
    "            fourier = np.abs(np.fft.fft(chunk[i, :]))\n",
    "            # Select the top k features based on their scores\n",
    "            selected_features = SelectKBest(score_func=f_classif, k=k)\n",
    "            selected_features.fit_transform(fourier.reshape(1, -1), np.array([label]))  # Assuming label is binary\n",
    "            selected_indices = selected_features.get_support(indices=True)\n",
    "            selected_fourier = fourier[selected_indices]\n",
    "            segg.append(selected_fourier)\n",
    "        final_concate = np.concatenate(segg, axis=0)\n",
    "        final_concate = final_concate.reshape(-1, 1)\n",
    "        final_concate = scaler.fit_transform(final_concate)\n",
    "        final_concate = final_concate.ravel()\n",
    "        feature.append((final_concate, label))\n",
    "    return feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equal_length(pos_class_train,neg_class_train):\n",
    "    if len(pos_class_train) > len(neg_class_train):\n",
    "        pos_class_train = pos_class_train[:len(neg_class_train)]\n",
    "    elif len(pos_class_train) < len(neg_class_train):\n",
    "        neg_class_train = neg_class_train[:len(pos_class_train)]\n",
    "    return pos_class_train,neg_class_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining all and preprocessing the whole data\n",
    "def preprocess(subject_name,indices,labels,number_of_subjects=14,train_percent=0.8,Band_Pass_Low_Range=0,Band_Pass_High_Range=50,trail_times=25,augmented_ratio=2):\n",
    "    limit=math.ceil(train_percent*number_of_subjects)\n",
    "    k=1\n",
    "    train_data=[]\n",
    "    test_data=[]\n",
    "    for subject_number in indices:\n",
    "        if exists('./Dataset/{}{}.edf'.format(subject_name,subject_number)):\n",
    "            file='./Dataset/{}{}.edf'.format(subject_name,subject_number)\n",
    "            raw=mne.io.read_raw_edf(file,verbose=0,preload=True,eog=None,exclude=(),stim_channel='auto')\n",
    "            raw=raw.pick_types(eeg=True)\n",
    "            raw=raw.filter(Band_Pass_Low_Range,Band_Pass_High_Range,iir_params=iir_params,method='iir')\n",
    "            new_raw=select_channel(raw)\n",
    "            #new_raw=normalize_eeg_data(new_raw)\n",
    "            chunks=select_features(new_raw,label=labels,recording_time=trail_times,augment_ratio=augmented_ratio)\n",
    "            if k<=limit:\n",
    "                train_data+=chunks\n",
    "            else:\n",
    "                test_data+=chunks\n",
    "            k=k+1\n",
    "    return train_data,test_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_class_train,pos_class_test=preprocess('h',indices=indice,labels=0,trail_times=recording_time,augmented_ratio=1,train_percent=0.8)\n",
    "neg_class_train,neg_class_test=preprocess('s',indices=indice,labels=1,trail_times=recording_time,augmented_ratio=1,train_percent=0.8)\n",
    "pos_class_test,neg_class_test=equal_length(pos_class_test,neg_class_test)\n",
    "testing_data=pos_class_test+neg_class_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using double backslashes\n",
    "with open('./samples/pos_class_train.pkl', 'wb') as f:\n",
    "    pickle.dump(pos_class_train, f)\n",
    "\n",
    "# Using double backslashes\n",
    "with open('./samples/neg_class_train.pkl', 'wb') as f:\n",
    "    pickle.dump(neg_class_train, f)\n",
    "\n",
    "with open('./samples/testing_data.pkl', 'wb') as f:\n",
    "    pickle.dump(testing_data, f)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
