{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取当前日期\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "outdir = 'outdir'\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "\n",
    "# 构建导出的pth文件名以及ONNX文件名\n",
    "pth_file_path = os.path.join(outdir, f'eeg_depression-best_model_{current_date}.pth')\n",
    "onnx_file_path = os.path.join(outdir, f\"eeg_depression-best_model_{current_date}.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train Loss: 0.5409842133522034 Test Accuracy: 0.6988108812510181\n",
      "Epoch: 1 Train Loss: 0.49743080139160156 Test Accuracy: 0.7063854047890535\n",
      "Epoch: 2 Train Loss: 0.3357866108417511 Test Accuracy: 0.7057338328718032\n",
      "Epoch: 3 Train Loss: 0.3512541949748993 Test Accuracy: 0.7151001791822772\n",
      "Epoch: 4 Train Loss: 0.5077861547470093 Test Accuracy: 0.7392897866101971\n",
      "Epoch: 5 Train Loss: 0.3912697732448578 Test Accuracy: 0.7300048867893794\n",
      "Epoch: 6 Train Loss: 0.46829667687416077 Test Accuracy: 0.7259325623065646\n",
      "Epoch: 7 Train Loss: 0.3620418310165405 Test Accuracy: 0.7341586577618505\n",
      "Epoch: 8 Train Loss: 0.5151393413543701 Test Accuracy: 0.7269099201824402\n",
      "Epoch: 9 Train Loss: 0.4151233732700348 Test Accuracy: 0.7245479719824076\n",
      "Epoch: 10 Train Loss: 0.40602409839630127 Test Accuracy: 0.7346473366997882\n",
      "Epoch: 11 Train Loss: 0.49229711294174194 Test Accuracy: 0.7300863332790357\n",
      "Epoch: 12 Train Loss: 0.4111693501472473 Test Accuracy: 0.7269099201824402\n",
      "Epoch: 13 Train Loss: 0.4453388750553131 Test Accuracy: 0.7260954552858772\n",
      "Epoch: 14 Train Loss: 0.5846662521362305 Test Accuracy: 0.7314709236031927\n",
      "Epoch: 15 Train Loss: 0.3896697759628296 Test Accuracy: 0.7295976543410979\n",
      "Epoch: 16 Train Loss: 0.49633124470710754 Test Accuracy: 0.7283759569962535\n",
      "Epoch: 17 Train Loss: 0.4438967704772949 Test Accuracy: 0.7366020524515393\n",
      "Epoch: 18 Train Loss: 0.4946221709251404 Test Accuracy: 0.7136341423684639\n",
      "Epoch: 19 Train Loss: 0.46748289465904236 Test Accuracy: 0.7324482814790683\n",
      "Epoch: 20 Train Loss: 0.400870144367218 Test Accuracy: 0.7322039420100994\n",
      "Epoch: 21 Train Loss: 0.4703344702720642 Test Accuracy: 0.7233262746375632\n",
      "Epoch: 22 Train Loss: 0.32918059825897217 Test Accuracy: 0.7364391594722267\n",
      "Epoch: 23 Train Loss: 0.42936766147613525 Test Accuracy: 0.7237335070858446\n",
      "Epoch: 24 Train Loss: 0.46762335300445557 Test Accuracy: 0.7336699788239127\n",
      "Epoch: 25 Train Loss: 0.390824556350708 Test Accuracy: 0.733751425313569\n",
      "Epoch: 26 Train Loss: 0.5395153760910034 Test Accuracy: 0.7225932562306565\n",
      "Epoch: 27 Train Loss: 0.41279348731040955 Test Accuracy: 0.7288646359341913\n",
      "Epoch: 28 Train Loss: 0.390877366065979 Test Accuracy: 0.7428734321550741\n",
      "Epoch: 29 Train Loss: 0.4827883839607239 Test Accuracy: 0.7352174621273823\n",
      "best_accuracy: 0.7428734321550741\n",
      "Confusion Matrix:\n",
      "[[4721 1418]\n",
      " [1833 4306]]\n"
     ]
    }
   ],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, positive_file, negative_file):\n",
    "        # Load positive samples\n",
    "        with open(positive_file, 'rb') as f:\n",
    "            self.positive_samples = pickle.load(f)\n",
    "        \n",
    "        # Load negative samples\n",
    "        with open(negative_file, 'rb') as f:\n",
    "            self.negative_samples = pickle.load(f)\n",
    "\n",
    "        # Ensure both sets have the same length\n",
    "        self.length = min(len(self.positive_samples), len(self.negative_samples))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Concatenate positive and negative samples to form a batch\n",
    "        subject, positive_feature, positive_label = self.positive_samples[idx % len(self.positive_samples)]\n",
    "        subject, negative_feature, negative_label = self.negative_samples[idx % len(self.negative_samples)]\n",
    "        return torch.tensor(positive_feature), torch.tensor(negative_feature)\n",
    "\n",
    "class SimpleDNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleDNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(128, 64)  # Input size 100, output size 64\n",
    "        self.dropout1 = nn.Dropout(p=0.3)  # Dropout with a probability of 0.5\n",
    "        self.fc2 = nn.Linear(64, 32)   # Input size 64, output size 32\n",
    "        self.dropout2 = nn.Dropout(p=0.3)  # Dropout with a probability of 0.5\n",
    "        self.fc3 = nn.Linear(32, 1)    # Input size 32, output size 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# Define paths to your data files\n",
    "train_positive_file = './samples/train_positive_samples.pkl'\n",
    "train_negative_file = './samples/train_negative_samples.pkl'\n",
    "test_positive_file = './samples/test_positive_samples.pkl'\n",
    "test_negative_file = './samples/test_negative_samples.pkl'\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CustomDataset(train_positive_file, train_negative_file)\n",
    "test_dataset = CustomDataset(test_positive_file, test_negative_file)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = SimpleDNN()\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)  # Adam optimizer\n",
    "\n",
    "best_accuracy = 0.0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(30):  # Train for 10 epochs\n",
    "    for batch_idx, (positive_batch, negative_batch) in enumerate(train_loader):\n",
    "        # Concatenate positive and negative batches\n",
    "        inputs = torch.cat((positive_batch, negative_batch), dim=0)\n",
    "        labels = torch.cat((torch.ones(positive_batch.size(0), 1), torch.zeros(negative_batch.size(0), 1)), dim=0)\n",
    "\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        outputs = model(inputs.float())  # Forward pass\n",
    "        loss = criterion(outputs, labels.float())  # Calculate loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "    # Testing loop\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_correct = 0\n",
    "    total_samples = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for positive_batch, negative_batch in test_loader:\n",
    "            inputs = torch.cat((positive_batch, negative_batch), dim=0)\n",
    "            labels = torch.cat((torch.ones(positive_batch.size(0), 1), torch.zeros(negative_batch.size(0), 1)), dim=0)\n",
    "            \n",
    "            outputs = model(inputs.float())\n",
    "            predicted = (outputs > 0.5).float()  # Threshold at 0.5\n",
    "            total_samples += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_predictions.extend(predicted.numpy())\n",
    "\n",
    "    accuracy = test_correct / total_samples\n",
    "    print(f'Epoch: {epoch} Train Loss: {loss} Test Accuracy: {accuracy}')\n",
    "\n",
    "    # Save the best model\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model.state_dict(), pth_file_path)\n",
    "\n",
    "        # Export the best model to ONNX\n",
    "        dummy_input = torch.randn(1, 128)\n",
    "        torch.onnx.export(model, dummy_input, onnx_file_path, input_names=['input'], output_names=['output'])\n",
    "\n",
    "print(f\"best_accuracy: {best_accuracy}\")\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load(pth_file_path))\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the predictions in each test subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test for Negative subjects:\n",
      "Subject: A08, Accuracy: 0.58\n",
      "Subject: A02, Accuracy: 0.73\n",
      "Subject: A22, Accuracy: 0.98\n",
      "Subject: A06, Accuracy: 0.97\n",
      "Subject: A10, Accuracy: 0.63\n",
      "Subject: A24, Accuracy: 0.47\n",
      "Subject: A30, Accuracy: 0.92\n",
      "Subject: A17, Accuracy: 0.96\n",
      "Subject: A28, Accuracy: 0.96\n",
      "\n",
      "Test for Positive subjects:\n",
      "Subject: 20240619_2A50, Accuracy: 0.95\n",
      "Subject: 20240710_2A28, Accuracy: 0.38\n",
      "Subject: 20240717_2A12, Accuracy: 0.61\n",
      "Subject: 20240626_2A26, Accuracy: 0.90\n",
      "Subject: 20240619_2A15, Accuracy: 0.85\n",
      "Subject: 20240710_2A30, Accuracy: 0.76\n",
      "Subject: 20240626_2A13, Accuracy: 0.19\n",
      "Subject: 20240710_2A05, Accuracy: 0.97\n",
      "Subject: 20240710_2A31, Accuracy: 0.77\n"
     ]
    }
   ],
   "source": [
    "# 加载 ONNX 模型\n",
    "# onnx_model_path = './outdir/eeg_depression-best_model_2024-06-27.onnx'\n",
    "# ort_session = ort.InferenceSession(onnx_model_path)\n",
    "\n",
    "def get_onnxfile():\n",
    "    outdir = './outdir'\n",
    "    onnx_files = glob.glob(os.path.join(outdir, '*.onnx'))\n",
    "    # 如果找到的文件不为空\n",
    "    if onnx_files:\n",
    "        # 按文件名排序\n",
    "        onnx_files.sort()\n",
    "\n",
    "        # 选择最后一个文件\n",
    "        last_onnx_file = onnx_files[-1]\n",
    "        return last_onnx_file\n",
    "\n",
    "onnx_model_path = get_onnxfile()\n",
    "ort_session = ort.InferenceSession(onnx_model_path)\n",
    "\n",
    "# 预测函数\n",
    "def predict(features):\n",
    "    inputs = {ort_session.get_inputs()[0].name: features}\n",
    "    outputs = ort_session.run(None, inputs)\n",
    "    return outputs[0]\n",
    "\n",
    "\n",
    "for filePath in [test_negative_file, test_positive_file]:\n",
    "    if filePath == test_negative_file:\n",
    "        print(\"\\nTest for Negative subjects:\")\n",
    "    else:\n",
    "        print(\"\\nTest for Positive subjects:\")\n",
    "\n",
    "    # 加载测试数据集\n",
    "    with open(filePath, 'rb') as f:\n",
    "        test_data = pickle.load(f)\n",
    "\n",
    "    # 计算每个 subject 的准确率\n",
    "    subject_results = defaultdict(list)\n",
    "\n",
    "    for subject, features, label in test_data:\n",
    "        features = np.array(features, dtype=np.float32)  # 确保 features 的数据类型与模型输入匹配\n",
    "        features = features.reshape(1, -1)  # 调整形状以匹配模型输入\n",
    "        prediction = predict(features)\n",
    "        predicted_label = (prediction >= 0.5).astype(int)  # 使用阈值 0.5 将预测概率转换为类标签\n",
    "        subject_results[subject].append((predicted_label, label))\n",
    "\n",
    "    subject_accuracies = {}\n",
    "    for subject, results in subject_results.items():\n",
    "        correct = sum(1 for pred, label in results if pred == label)\n",
    "        accuracy = correct / len(results)\n",
    "        subject_accuracies[subject] = accuracy\n",
    "\n",
    "    # 输出每个 subject 的准确度\n",
    "    for subject, accuracy in subject_accuracies.items():\n",
    "        print(f'Subject: {subject}, Accuracy: {accuracy:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
