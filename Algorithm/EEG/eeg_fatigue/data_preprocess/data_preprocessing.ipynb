{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os \n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "morning = r'C:\\Users\\Administrator\\Desktop\\our_fatigue\\Morning'\n",
    "evening = r'C:\\Users\\Administrator\\Desktop\\our_fatigue\\Evening'\n",
    "label = r'C:\\Users\\Administrator\\Desktop\\our_fatigue\\label.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = pd.read_csv(label)  # IF Morning Fatigue < Evening Fatigue ---> Label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_json_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def read_eeg_data(folder):\n",
    "    data = {}\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith(\".json\"): \n",
    "            subject_id = filename.split('_')[0]\n",
    "            file_path = os.path.join(folder, filename)\n",
    "            json_data = read_json_file(file_path)\n",
    "            left_wave_all, right_wave_all = [], []\n",
    "            for raw_data in json_data['ProcessedDatas']:\n",
    "                left_wave = raw_data['LeftWave']\n",
    "                right_wave = raw_data['RightWave']\n",
    "                if any(np.isnan(left_wave)) or any(np.isnan(right_wave)):\n",
    "                    print(f\"NaNs detected in raw data for subject {subject_id}\")\n",
    "\n",
    "\n",
    "                left_wave_all += left_wave\n",
    "                right_wave_all += right_wave\n",
    "            final_data = np.column_stack((left_wave_all,right_wave_all))\n",
    "            final_data = final_data[900:]\n",
    "            if np.isnan(final_data).any():\n",
    "                print(f\"NaNs detected in final data for subject {subject_id}\")\n",
    "\n",
    "            data[subject_id] = final_data\n",
    "    return data\n",
    "\n",
    "def equalize_length(data1, data2):\n",
    "    min_length = min(len(data1), len(data2))\n",
    "    return data1[:min_length], data2[:min_length]\n",
    "\n",
    "\n",
    "\n",
    "def split_into_chunks(data, chunk_size):\n",
    "    num_chunks = data.shape[0] // chunk_size\n",
    "    return data[:num_chunks * chunk_size,:].reshape(num_chunks,2, chunk_size)\n",
    "\n",
    "\n",
    "\n",
    "def apply_fft(data):\n",
    "    feat = []\n",
    "    for i in range(data.shape[0]):\n",
    "        chunk = data[i]\n",
    "        ch_feat = []\n",
    "        for channel in chunk:\n",
    "            fft_data = np.abs(np.fft.fft(channel))[:64]\n",
    "            sum = np.sum(fft_data)\n",
    "            if sum == 0:\n",
    "                print('sum is zero')\n",
    "            fft_data = fft_data / sum\n",
    "            ch_feat.extend(fft_data)\n",
    "        feat.append(ch_feat)\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Administrator\\\\Desktop\\\\our_fatigue\\\\Morning'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "morning_data = read_eeg_data(morning)\n",
    "evening_data = read_eeg_data(evening)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitting(subjects,labels, morning= morning,evening = evening):\n",
    "    morning_data = read_eeg_data(morning)\n",
    "    evening_data = read_eeg_data(evening)\n",
    "    eeg_data = {}\n",
    "    for subject in subjects:\n",
    "        morning_data_subject = np.array(morning_data[subject])\n",
    "        evening_data_subject = np.array(evening_data[subject])\n",
    "        \n",
    "        # Equalize length\n",
    "        morning_data_subject, evening_data_subject = equalize_length(morning_data_subject, evening_data_subject)\n",
    "        \n",
    "        eeg_data[subject] = {\n",
    "            'morning': apply_fft(split_into_chunks(morning_data_subject, 225)),\n",
    "            'evening': apply_fft(split_into_chunks(evening_data_subject, 225)),\n",
    "            'label': labels[subject]\n",
    "        }\n",
    "    return eeg_data\n",
    "\n",
    "\n",
    "#subjects = list(set(morning_data.keys()).intersection(evening_data.keys()))\n",
    "train_positive = ['A06','A04']  # Label = 0\n",
    "train_negative = ['A05','A03'] # Label = 1\n",
    "test_positive = ['A02'] # label = 1\n",
    "test_negative = ['A01'] # label = 0\n",
    "\n",
    "\n",
    "train_data = train_positive + train_negative\n",
    "test_data = test_negative + test_positive\n",
    "labels = {subject: 1 for subject in train_positive + test_positive}\n",
    "labels.update({subject: 0 for subject in train_negative + test_negative})\n",
    "\n",
    "\n",
    "train_eeg_data = splitting(train_data,labels= labels)\n",
    "test_eeg_data = splitting(test_data,labels= labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_check(data, label):\n",
    "    total = []\n",
    "    for i in label:\n",
    "        dat = data[i]['morning']\n",
    "        total.extend(dat)\n",
    "    total = np.array(total)\n",
    "    return len(total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train positive  506\n",
      "Length of train negative  638\n",
      "Length of test positive  316\n",
      "Length of test negative  318\n"
     ]
    }
   ],
   "source": [
    "length_of_train_positve = length_check(train_eeg_data,train_positive)\n",
    "length_of_train_negative = length_check(train_eeg_data,train_negative)\n",
    "length_of_test_positve = length_check(test_eeg_data,test_positive)\n",
    "length_of_test_negative = length_check(test_eeg_data,test_negative)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Length of train positive \",length_of_train_positve )\n",
    "print(\"Length of train negative \",length_of_train_negative )\n",
    "print(\"Length of test positive \",length_of_test_positve )\n",
    "print(\"Length of test negative \",length_of_test_negative )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class CustomEEGDataset(Dataset):\n",
    "    def __init__(self, eeg_data):\n",
    "        self.data = []\n",
    "        for subject in eeg_data:\n",
    "            morning_chunks = eeg_data[subject]['morning']\n",
    "            evening_chunks = eeg_data[subject]['evening']\n",
    "            label = eeg_data[subject]['label']\n",
    "            \n",
    "            # Assuming morning_chunks and evening_chunks are lists of shape [316, 128]\n",
    "            for i in range(len(morning_chunks)):\n",
    "                morning_chunk = torch.tensor(morning_chunks[i], dtype=torch.float32)\n",
    "                evening_chunk = torch.tensor(evening_chunks[i], dtype=torch.float32)\n",
    "                self.data.append((subject, morning_chunk, evening_chunk, torch.tensor(label, dtype=torch.float32)))\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CustomEEGDataset(train_eeg_data)\n",
    "test_dataset = CustomEEGDataset(test_eeg_data)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject shape: ('A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06', 'A06')\n",
      "Input1 shape: torch.Size([128, 128])\n",
      "Input2 shape: torch.Size([128, 128])\n",
      "Label shape: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    sub, input1, input2, label = batch\n",
    "    print(f\"subject shape: {sub}\")\n",
    "    print(f\"Input1 shape: {input1.shape}\")\n",
    "    print(f\"Input2 shape: {input2.shape}\")\n",
    "    print(f\"Label shape: {label.shape}\")\n",
    "    break  # Remove this break to inspect all batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import optim\n",
    "class RankNet(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(RankNet, self).__init__()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "outdir = 'outdir'\n",
    "pth_file_path = os.path.join(outdir, f'testing-best_model_{current_date}.pth')\n",
    "onnx_file_path = os.path.join(outdir, f\"eeg_testing-best_model_{current_date}.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train Loss: 0.6893543601036072 Test Accuracy: 0.48264984227129337\n",
      "Best Accuracy Model: 0.48264984227129337\n",
      "Epoch: 1 Train Loss: 0.6190316081047058 Test Accuracy: 0.6009463722397477\n",
      "Best Accuracy Model: 0.6009463722397477\n",
      "Epoch: 2 Train Loss: 0.3601899743080139 Test Accuracy: 0.5993690851735016\n",
      "Epoch: 3 Train Loss: 0.31114310026168823 Test Accuracy: 0.6451104100946372\n",
      "Best Accuracy Model: 0.6451104100946372\n",
      "Epoch: 4 Train Loss: 0.2976534068584442 Test Accuracy: 0.637223974763407\n",
      "Epoch: 5 Train Loss: 0.2875290513038635 Test Accuracy: 0.6198738170347003\n",
      "Epoch: 6 Train Loss: 0.2866315543651581 Test Accuracy: 0.6246056782334385\n",
      "Epoch: 7 Train Loss: 0.2836405634880066 Test Accuracy: 0.6324921135646687\n",
      "Epoch: 8 Train Loss: 0.27383777499198914 Test Accuracy: 0.6324921135646687\n",
      "Epoch: 9 Train Loss: 0.271921843290329 Test Accuracy: 0.6356466876971609\n",
      "Epoch: 10 Train Loss: 0.2688020169734955 Test Accuracy: 0.6309148264984227\n",
      "Epoch: 11 Train Loss: 0.26468032598495483 Test Accuracy: 0.6324921135646687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 Train Loss: 0.2620687782764435 Test Accuracy: 0.6277602523659306\n",
      "Epoch: 13 Train Loss: 0.25606799125671387 Test Accuracy: 0.6277602523659306\n",
      "Epoch: 14 Train Loss: 0.2508523464202881 Test Accuracy: 0.613564668769716\n",
      "Epoch: 15 Train Loss: 0.25124606490135193 Test Accuracy: 0.6230283911671924\n",
      "Epoch: 16 Train Loss: 0.2447555810213089 Test Accuracy: 0.613564668769716\n",
      "Epoch: 17 Train Loss: 0.24286018311977386 Test Accuracy: 0.6072555205047319\n",
      "Epoch: 18 Train Loss: 0.2361968606710434 Test Accuracy: 0.613564668769716\n",
      "Epoch: 19 Train Loss: 0.23514172434806824 Test Accuracy: 0.5851735015772871\n",
      "Epoch: 20 Train Loss: 0.2252461314201355 Test Accuracy: 0.5914826498422713\n",
      "Epoch: 21 Train Loss: 0.22587554156780243 Test Accuracy: 0.580441640378549\n",
      "Epoch: 22 Train Loss: 0.23001471161842346 Test Accuracy: 0.5741324921135647\n",
      "Epoch: 23 Train Loss: 0.2180643379688263 Test Accuracy: 0.5851735015772871\n",
      "Epoch: 24 Train Loss: 0.21771186590194702 Test Accuracy: 0.5725552050473186\n",
      "Epoch: 25 Train Loss: 0.2110796570777893 Test Accuracy: 0.5757097791798107\n",
      "Epoch: 26 Train Loss: 0.2126743048429489 Test Accuracy: 0.5694006309148265\n",
      "Epoch: 27 Train Loss: 0.20968040823936462 Test Accuracy: 0.5488958990536278\n",
      "Epoch: 28 Train Loss: 0.20925989747047424 Test Accuracy: 0.5457413249211357\n",
      "Epoch: 29 Train Loss: 0.20456533133983612 Test Accuracy: 0.5488958990536278\n",
      "Epoch: 30 Train Loss: 0.20372945070266724 Test Accuracy: 0.5331230283911672\n",
      "Epoch: 31 Train Loss: 0.20901824533939362 Test Accuracy: 0.5331230283911672\n",
      "Epoch: 32 Train Loss: 0.19347891211509705 Test Accuracy: 0.5283911671924291\n",
      "Epoch: 33 Train Loss: 0.19821466505527496 Test Accuracy: 0.5252365930599369\n",
      "Epoch: 34 Train Loss: 0.1894950568675995 Test Accuracy: 0.5331230283911672\n",
      "Epoch: 35 Train Loss: 0.1783321052789688 Test Accuracy: 0.5520504731861199\n",
      "Epoch: 36 Train Loss: 0.17852383852005005 Test Accuracy: 0.5615141955835962\n",
      "Epoch: 37 Train Loss: 0.21755722165107727 Test Accuracy: 0.5236593059936908\n",
      "Epoch: 38 Train Loss: 0.23406201601028442 Test Accuracy: 0.49053627760252366\n",
      "Epoch: 39 Train Loss: 0.16663891077041626 Test Accuracy: 0.5347003154574133\n",
      "Epoch: 40 Train Loss: 0.1754947006702423 Test Accuracy: 0.5425867507886435\n",
      "Epoch: 41 Train Loss: 0.19283528625965118 Test Accuracy: 0.526813880126183\n",
      "Epoch: 42 Train Loss: 0.174153670668602 Test Accuracy: 0.5536277602523659\n",
      "Epoch: 43 Train Loss: 0.20031854510307312 Test Accuracy: 0.5299684542586751\n",
      "Epoch: 44 Train Loss: 0.22643476724624634 Test Accuracy: 0.49369085173501576\n",
      "Epoch: 45 Train Loss: 0.17185701429843903 Test Accuracy: 0.5157728706624606\n",
      "Epoch: 46 Train Loss: 0.16811971366405487 Test Accuracy: 0.5441640378548895\n",
      "Epoch: 47 Train Loss: 0.16296866536140442 Test Accuracy: 0.5378548895899053\n",
      "Epoch: 48 Train Loss: 0.17427439987659454 Test Accuracy: 0.5220820189274448\n",
      "Epoch: 49 Train Loss: 0.1545325070619583 Test Accuracy: 0.526813880126183\n",
      "Epoch: 50 Train Loss: 0.19175980985164642 Test Accuracy: 0.5283911671924291\n",
      "Epoch: 51 Train Loss: 0.18839998543262482 Test Accuracy: 0.5315457413249212\n",
      "Epoch: 52 Train Loss: 0.21970075368881226 Test Accuracy: 0.5126182965299685\n",
      "Epoch: 53 Train Loss: 0.20262272655963898 Test Accuracy: 0.501577287066246\n",
      "Epoch: 54 Train Loss: 0.15186724066734314 Test Accuracy: 0.5315457413249212\n",
      "Epoch: 55 Train Loss: 0.16492576897144318 Test Accuracy: 0.5520504731861199\n",
      "Epoch: 56 Train Loss: 0.19202972948551178 Test Accuracy: 0.5441640378548895\n",
      "Epoch: 57 Train Loss: 0.1392989605665207 Test Accuracy: 0.526813880126183\n",
      "Epoch: 58 Train Loss: 0.18934334814548492 Test Accuracy: 0.5441640378548895\n",
      "Epoch: 59 Train Loss: 0.19693657755851746 Test Accuracy: 0.48580441640378547\n",
      "Epoch: 60 Train Loss: 0.15316982567310333 Test Accuracy: 0.5394321766561514\n",
      "Epoch: 61 Train Loss: 0.1497366726398468 Test Accuracy: 0.5394321766561514\n",
      "Epoch: 62 Train Loss: 0.1549733430147171 Test Accuracy: 0.5220820189274448\n",
      "Epoch: 63 Train Loss: 0.14241036772727966 Test Accuracy: 0.5189274447949527\n",
      "Epoch: 64 Train Loss: 0.1529233455657959 Test Accuracy: 0.5236593059936908\n",
      "Epoch: 65 Train Loss: 0.13864614069461823 Test Accuracy: 0.5236593059936908\n",
      "Epoch: 66 Train Loss: 0.149595707654953 Test Accuracy: 0.5\n",
      "Epoch: 67 Train Loss: 0.16815954446792603 Test Accuracy: 0.5189274447949527\n",
      "Epoch: 68 Train Loss: 0.17498630285263062 Test Accuracy: 0.5205047318611987\n",
      "Epoch: 69 Train Loss: 0.19898954033851624 Test Accuracy: 0.5252365930599369\n",
      "Epoch: 70 Train Loss: 0.16335028409957886 Test Accuracy: 0.5394321766561514\n",
      "Epoch: 71 Train Loss: 0.15393906831741333 Test Accuracy: 0.5141955835962145\n",
      "Epoch: 72 Train Loss: 0.13420748710632324 Test Accuracy: 0.5141955835962145\n",
      "Epoch: 73 Train Loss: 0.12881572544574738 Test Accuracy: 0.5252365930599369\n",
      "Epoch: 74 Train Loss: 0.1301099956035614 Test Accuracy: 0.5410094637223974\n",
      "Epoch: 75 Train Loss: 0.14114366471767426 Test Accuracy: 0.5283911671924291\n",
      "Epoch: 76 Train Loss: 0.13642624020576477 Test Accuracy: 0.5347003154574133\n",
      "Epoch: 77 Train Loss: 0.1468788981437683 Test Accuracy: 0.5362776025236593\n",
      "Epoch: 78 Train Loss: 0.13269728422164917 Test Accuracy: 0.5315457413249212\n",
      "Epoch: 79 Train Loss: 0.13640183210372925 Test Accuracy: 0.5141955835962145\n",
      "Epoch: 80 Train Loss: 0.21202589571475983 Test Accuracy: 0.5236593059936908\n",
      "Epoch: 81 Train Loss: 0.08822937309741974 Test Accuracy: 0.5220820189274448\n",
      "Epoch: 82 Train Loss: 0.13922499120235443 Test Accuracy: 0.5094637223974764\n",
      "Epoch: 83 Train Loss: 0.13982635736465454 Test Accuracy: 0.5157728706624606\n",
      "Epoch: 84 Train Loss: 0.16875244677066803 Test Accuracy: 0.5141955835962145\n",
      "Epoch: 85 Train Loss: 0.14174631237983704 Test Accuracy: 0.5425867507886435\n",
      "Epoch: 86 Train Loss: 0.12902894616127014 Test Accuracy: 0.5047318611987381\n",
      "Epoch: 87 Train Loss: 0.12969288229942322 Test Accuracy: 0.5157728706624606\n",
      "Epoch: 88 Train Loss: 0.11037526279687881 Test Accuracy: 0.5299684542586751\n",
      "Epoch: 89 Train Loss: 0.09468341618776321 Test Accuracy: 0.5205047318611987\n",
      "Epoch: 90 Train Loss: 0.10087011009454727 Test Accuracy: 0.5126182965299685\n",
      "Epoch: 91 Train Loss: 0.12080977857112885 Test Accuracy: 0.5110410094637224\n",
      "Epoch: 92 Train Loss: 0.2241000235080719 Test Accuracy: 0.5299684542586751\n",
      "Epoch: 93 Train Loss: 0.10246729850769043 Test Accuracy: 0.5504731861198738\n",
      "Epoch: 94 Train Loss: 0.10804592072963715 Test Accuracy: 0.5220820189274448\n",
      "Epoch: 95 Train Loss: 0.11574196815490723 Test Accuracy: 0.5126182965299685\n",
      "Epoch: 96 Train Loss: 0.11581934988498688 Test Accuracy: 0.5283911671924291\n",
      "Epoch: 97 Train Loss: 0.1215471401810646 Test Accuracy: 0.5126182965299685\n",
      "Epoch: 98 Train Loss: 0.09762527048587799 Test Accuracy: 0.5252365930599369\n",
      "Epoch: 99 Train Loss: 0.08580294996500015 Test Accuracy: 0.5252365930599369\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "def rank_loss(output1, output2, label):\n",
    "    # Subtract scores\n",
    "    diff = output1 - output2\n",
    "    prob = torch.sigmoid(diff).squeeze()\n",
    "    assert (0 <= prob).all() and (prob <= 1).all(), \"Probabilities are not in [0, 1] range\"\n",
    "    loss = F.binary_cross_entropy(prob, label)\n",
    "    return loss\n",
    "\n",
    "def rank_accuracy(output1, output2, label):\n",
    "    # Compute the predicted ranking (0 or 1)\n",
    "    #print(\"Input shape \", output1.shape)\n",
    "    diff = output1 - output2\n",
    "    #print(\"diff_shape\", diff.shape)\n",
    "    prob = torch.sigmoid(diff).squeeze()\n",
    "    #print('prob',prob.shape)\n",
    "    pred_label = (prob > 0.5)\n",
    "    #print('pred_label',pred_label.shape)\n",
    "    test_correct = (pred_label == label).float().sum()\n",
    "    #print(\"test_correct\",test_correct.shape, test_correct)\n",
    "    return test_correct\n",
    "\n",
    "\n",
    "# Training function\n",
    "def train(model, data_loader, test_loader, optimizer, epochs):\n",
    "    #model.train()\n",
    "    best_accuracy = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        #total_accuracy = 0\n",
    "        for _,input1, input2, label in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output1 = model(input1)\n",
    "            output2 = model(input2)\n",
    "            \n",
    "            loss = rank_loss(output1, output2, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        test_correct = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for _,t_in1,t_in2,lab in test_loader:\n",
    "                out1 = model(t_in1)\n",
    "                out2 = model(t_in2)\n",
    "                total_samples += lab.size(0)\n",
    "                \n",
    "                acc = rank_accuracy(out1,out2,lab)\n",
    "                test_correct += acc.item()\n",
    "                #print('final test_correct',test_correct)\n",
    "        #print('total_samples',total_samples)   \n",
    "        accuracy = test_correct / total_samples\n",
    "        print(f'Epoch: {epoch} Train Loss: {loss} Test Accuracy: {accuracy}')\n",
    "        # Save the best model\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save(model.state_dict(), pth_file_path)\n",
    "            print(\"Best Accuracy Model: {}\".format(best_accuracy))\n",
    "\n",
    "            # Export the best model to ONNX\n",
    "            dummy_input = torch.randn(1, 128)\n",
    "            torch.onnx.export(model, dummy_input, onnx_file_path, input_names=['input'], output_names=['output'])\n",
    "\n",
    "input_size = 128\n",
    "model = RankNet(input_size)  # Ensure your model outputs match input_size\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Start training\n",
    "train(model, train_loader,test_loader, optimizer, epochs=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import onnxruntime as ort\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_onnxfile():\n",
    "    outdir = './outdir'\n",
    "    onnx_files = glob.glob(os.path.join(outdir, '*.onnx'))\n",
    "    if onnx_files:\n",
    "        onnx_files.sort()\n",
    "        last_onnx_file = onnx_files[-1]\n",
    "        return last_onnx_file\n",
    "    return None\n",
    "\n",
    "onnx_model_path = get_onnxfile()\n",
    "if onnx_model_path is None:\n",
    "    raise FileNotFoundError(\"No ONNX model file found in the specified directory.\")\n",
    "\n",
    "# Load the ONNX model\n",
    "ort_session = ort.InferenceSession(onnx_model_path)\n",
    "\n",
    "# Prediction function\n",
    "def predict(features):\n",
    "    inputs = {ort_session.get_inputs()[0].name: features}\n",
    "    outputs = ort_session.run(None, inputs)\n",
    "    return outputs[0]\n",
    "\n",
    "# Helper function to predict probabilities\n",
    "def predict_probabilities(features):\n",
    "    prediction = predict(features)\n",
    "    return torch.sigmoid(torch.from_numpy(prediction)).squeeze().item()\n",
    "\n",
    "# Initialize results dictionary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing(train_loader):\n",
    "    subject_results = defaultdict(list)\n",
    "    # Iterate through the train loader and calculate predictions\n",
    "    for subject, features1_batch, features2_batch, label_batch in train_loader:\n",
    "        for i in range(features1_batch.shape[0]):\n",
    "            features1 = features1_batch[i].numpy().astype(np.float32).reshape(1, -1)\n",
    "            features2 = features2_batch[i].numpy().astype(np.float32).reshape(1, -1)\n",
    "            label = label_batch[i].item()\n",
    "\n",
    "            p1 = predict_probabilities(features1)\n",
    "            p2 = predict_probabilities(features2)\n",
    "            \n",
    "            diff = predict(features1) - predict(features2)\n",
    "            prob = torch.sigmoid(torch.from_numpy(diff)).squeeze().item()\n",
    "            pred_label = (prob > 0.5)\n",
    "            \n",
    "            subject_results[subject].append((pred_label, label, [p1, p2]))\n",
    "\n",
    "    # Combine subject results into a new structure\n",
    "    new_subject_results = defaultdict(list)\n",
    "    for subjects, pairs in subject_results.items():\n",
    "        if isinstance(subjects, tuple):\n",
    "            for subject in subjects:\n",
    "                if subject:\n",
    "                    new_subject_results[subject].extend(pairs)\n",
    "        else:\n",
    "            new_subject_results[subjects].extend(pairs)\n",
    "\n",
    "    # Convert defaultdict to a regular dictionary if needed\n",
    "    new_subject_results = dict(new_subject_results)\n",
    "\n",
    "    # Calculate and print accuracy for each subject\n",
    "    for subject, results in new_subject_results.items():\n",
    "        if results:  # Ensure there are results to calculate accuracy\n",
    "            correct = sum(pred == true for pred, true, _ in results)\n",
    "            accuracy = correct / len(results)\n",
    "            print(f'Subject: {subject}, Accuracy: {accuracy:.2f}')\n",
    "        else:\n",
    "            print(f'Subject: {subject} has no results, accuracy cannot be calculated.')\n",
    "    p1_f = 0\n",
    "    p2_f = 0\n",
    "    subject_features = defaultdict(list)\n",
    "    # Iterate over each result\n",
    "    for subject, feat in new_subject_results.items():\n",
    "        p1_f = 0\n",
    "        p2_f = 0\n",
    "        for _, _, features in feat:\n",
    "            # Debug: Print the features to understand its structure\n",
    "            #print(\"Features:\", features)\n",
    "            if features[0] > features[1]:\n",
    "                p1_f += 1\n",
    "            else:\n",
    "                p2_f +=1\n",
    "        subject_features[subject].append({'Morning': p1_f, 'Evening': p2_f})\n",
    "    print(dict(subject_features))\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "train_positive = ['A06','A04']  # Label = 0\n",
    "\n",
    "train_negative = ['A05','A03'] # Label = 1\n",
    "\n",
    "test_positive = ['A02'] # label = 1\n",
    "\n",
    "test_negative = ['A01'] # label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: A06, Accuracy: 0.39\n",
      "Subject: A04, Accuracy: 0.72\n",
      "Subject: A05, Accuracy: 0.66\n",
      "Subject: A03, Accuracy: 0.84\n",
      "{'A06': [{'Morning': 8904, 'Evening': 14136}], 'A04': [{'Morning': 30512, 'Evening': 11216}], 'A05': [{'Morning': 14288, 'Evening': 26416}], 'A03': [{'Morning': 6336, 'Evening': 33664}]}\n"
     ]
    }
   ],
   "source": [
    "processing(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: A01, Accuracy: 0.62\n",
      "Subject: A02, Accuracy: 0.67\n",
      "{'A01': [{'Morning': 15318, 'Evening': 25386}], 'A02': [{'Morning': 25468, 'Evening': 14248}]}\n"
     ]
    }
   ],
   "source": [
    "processing(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
